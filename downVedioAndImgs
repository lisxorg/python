#!/usr/bin/env python
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import urllib.request,re,os
import json

#信息的保存路径
targetPathVedio = "D:\\resources\\crawler\\vedios"
targetPathImg = "D:\\resources\\crawler\\images"
targetPathFile = "D:\\resources\\crawler\\files"
#分页信息
page = 1;
# 网址  
baseurl = "http://so.ku6.com/search?q=搞笑"
#伪装浏览器访问，避免某些站点的反爬虫
headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '                            'Chrome/51.0.2704.63 Safari/537.36'  
           }

#下载资源并保存
def downAndSave(src,allPath):
    
    print('开始下载文件：'+allPath)
    try:
        print('正在下载文件：......')
        if src.startswith('http://'):
            urllib.request.urlretrieve(src,allPath)
        else:
            urllib.request.urlretrieve('http:'+src,allPath)
    except Exception as e:
        print("文件下载发生错误："+str(e))
    
    print("文件下载完成："+allPath)
    
#保存页面内容
def savePage(soup,title):
    page = soup.prettify()   
    f = open(targetPathFile+"\\"+title+".text", "w",encoding='utf-8')
    f.write(page)
    #关闭打开的文件
    f.close()      
    
#解析内容
def parseAndSaveCon(aHref):

        req = urllib.request.Request(url=aHref, headers=headers)
        res = urllib.request.urlopen(req)
        data = res.read().decode('gbk')
        flashVars = re.findall(r'\'flashVars\':.*?;',data)
        vid=flashVars[0].split('&')[5][4:]
        print('解析的vid：'+vid)
        #此urlDiv不做过多说明，你需要破解object，然后根据代码找出相关内容。如有疑问，可以联系我。
        urlDiv='http://'
        print('即将请求数据：URL'+urlDiv)
        reqVid = urllib.request.Request(url=urlDiv, headers=headers)
        resVid = urllib.request.urlopen(reqVid)
        dataVid = resVid.read().decode('gbk')
        vidInfo = json.loads(dataVid)
        print(vidInfo)
        #'picpath': 'http://i99.ku6.com/20094/25/21/1243538633756/2.jpg',        
        vedioUrl=vidInfo.get("data").get("f")        
        #print("解析的视频地址："+vedioUrl)
        vedioTitle=vidInfo.get("data").get("t")
        #查询且保存视频图片
        imgUrl=vidInfo.get("data").get("picpath")  
        suffix = imgUrl.rindex('.')
        imgSaveName = vedioTitle+imgUrl[suffix:]
        imgAllPath = os.path.join(targetPathImg,imgSaveName)
        
        startName = vedioUrl.rindex('.')
        try:
            endName = vedioUrl.rindex('?')    
            VedioSaveName = vedioTitle+vedioUrl[startName:endName]                
                        
        except Exception :
            VedioSaveName = vedioTitle+vedioUrl[startName:]
                
        vedioAllPath = os.path.join(targetPathVedio,VedioSaveName)
        #下载保存视频
        print("准备下载视频："+vedioAllPath)
        downAndSave(vedioUrl,vedioAllPath)
        #下载保存视频图片
        print("准备下载图片："+imgAllPath)
        downAndSave(imgUrl,imgAllPath)
        print("》》》》》》》》》》》》》》》》《《《《《《《《《《《《《《《")

def saveContets(imgTaglist):
    for img in imgTaglist:
        #获取此img的视频播放地址
        aHref = img.parent.get('href')
        print("即将解析页面："+ aHref)
        parseAndSaveCon(aHref)

if __name__ == '__main__':
    print('》》》》》》》》》》》将要执行酷6网资源爬取操作，预备工作已经完成。《《《《《《《《《《《《《')
    while True:
        url = baseurl+"&start="+str(page)
        print('进行分页查询，当前分页：'+str(page)+'。当前url格式:'+url)
        req = urllib.request.Request(url=url, headers=headers)
        res = urllib.request.urlopen(req)  
        data = res.read().decode('utf-8')
        soup = BeautifulSoup(data)
        #img的数据比较准确，如果直接获取a的数量进行操作是不准确及难处理的。
        imgTaglist=soup.find_all(name='div',attrs={'class':'ckl_cotcent'})[0].find_all(name='img')
        #计算集合的个数
        imglistLength = len(imgTaglist)
        #资源的数量
        print('size：'+str(imglistLength))
        if imglistLength<=0:
            print('此url查询内容为NULL，将结束查询:'+url)
            break
        saveContets(imgTaglist)
        page+=1
