# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import urllib.request,socket,re,sys,os  
import uuid
from symbol import except_clause

targetPath = "D:\\projects\\images"
#分页信息
page = 91;
# 网址  
baseurl = "http://so.ku6.com/search?q=%E7%A7%91%E5%AD%A6%E5%AE%9E%E9%AA%8C" 
headers = {  
              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '                            'Chrome/51.0.2704.63 Safari/537.36'  
           }

#下载资源并保存
def downAndSave(src,allPath):
    try:
        if src.startswith('http://'):
            urllib.request.urlretrieve(src,allPath)
        else:
            urllib.request.urlretrieve('http:'+src,allPath)
    except BaseException:
        print('str(e):\t\t', str(BaseException))

#解析相关的内容并保存
def searchContents(imgTaglist):
   
    for img in imgTaglist:  
        src=img.get('src')
        alt=img.get('alt')   
        orgiName = src.rindex('/')
        reName = str(alt)+str(uuid.uuid1())+src[orgiName+1:]
        print('正在下载文件：'+reName)
        allPath = os.path.join(targetPath,reName)
        #下载保存文件
        downAndSave(src,allPath)
        
def searchVedio(aTaglist):
    for img in aTaglist:  
        href=img.get('href')
        req = urllib.request.Request(url=href, headers=headers)
        res = urllib.request.urlopen(req)
        data = res.read().decode('gbk')
        vedioSoup = BeautifulSoup(data)
        objectVedio=vedioSoup.find_all(name='object')
        if objectVedio != None:
            #urlVedio = objectVedio[1].data
            print(objectVedio);
        #下载保存文件
        #downAndSave(href,allPath)
    
print('》》》》》》》》》》》将要执行酷6网资源爬取操作，预备工作已经完成。《《《《《《《《《《《《《')
while True:
    url = baseurl+"&start="+str(page)
    print('进行分页查询，当前分页：'+str(page)+'。当前url格式:'+url)
    req = urllib.request.Request(url=url, headers=headers)
    res = urllib.request.urlopen(req)  
    data = res.read().decode('utf-8')
    soup = BeautifulSoup(data)
    imgTaglist=soup.find_all(name='div',attrs={'class':'ckl_cotcent'})[0].find_all(name='img')
    aTaglist=soup.find_all(name='div',attrs={'class':'ckl_cotcent'})[0].find_all(name='a')
    #计算集合的个数,a标签的数量和img标签是相同的
    alistLength = len(imgTaglist)
    #资源的数量
    print('size：'+str(alistLength))
    if alistLength<=0:
        print('此url查询内容为NULL，将结束查询:'+url)
        break
    searchContents(imgTaglist)
    searchVedio(aTaglist)
    page+=1


    
#print(type(soup))
#page = soup.prettify()
#print(page)
# 关闭打开的文件
#f = open(targetPath+"\\page.text", "w",encoding='utf-8')
#f.write(page)
#f.close()



#     src1=img.get('data-src')
#     print(src1)
#     if src1!=None:
#         pos1 = src1.rindex('/')
#         t1 = os.path.join(targetPath,src1[pos1+1:])
#         try:
#             if src1.startswith('http://'):
#                 urllib.request.urlretrieve(src1,t1)
#             else:
#                 urllib.request.urlretrieve('http:'+src1,t1)
#         except BaseException:
#             print('str(e):\t\t', str(BaseException))
        
